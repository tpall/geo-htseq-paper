---
title: A field-wide assessment of differential high throughput sequencing reveals widespread bias
preprint: true
authors: 
  - name: Taavi Päll
    affiliation: Institute of Biomedicine and Translational Medicine, University of Tartu, Estonia
    department: Department of Microbiology
    location: Ravila 19, 50411, Tartu, Estonia
    email: taavi.pall@ut.ee
  - name: Hannes Luidalepp
    affiliation: Quretec (https://www.quretec.com)
    location: Ülikooli 6a, 51003, Tartu, Estonia
    email: luidale@gmail.com
  - name: Tanel Tenson
    affiliation: Institute of Technology, University of Tartu, Estonia
    location: Nooruse 1, 50411, Tartu, Estonia
    email: tanel.tenson@ut.ee
  - name: Ülo Maiväli
    affiliation: Institute of Technology, University of Tartu, Estonia
    location: Nooruse 1, 50411, Tartu, Estonia
    thanks: corresponding author
    email: ymaivali@gmail.com
abstract: |
  Here we assess reproducibility and inferential quality in the field of differential HT-seq, based on analysis of datasets submitted 2008-2019 to the NCBI GEO data repository. Analysis of GEO submission file structures places an overall 59% upper limit to reproducibility. We further show that only 23% of experiments resulted in theoretically expected p value histogram shapes, although both reproducibility and p value distributions show marked improvement over time. Uniform p value histogram shapes, indicative of <100 true effects, were extremely few. Our calculations of $\pi_0$, the fraction of true nulls, showed that 36% of experiments have $\pi_0$ <0.5, meaning that in over a third of experiments most RNA-s were estimated to change their expression level upon experimental treatment. Both the fraction of different p value histogram types and $\pi_0$  values are strongly associated with the software used for calculating these p values by the original authors, indicating widespread bias.
header-includes: >
  \usepackage{lipsum}
bibliography: references.bib
output:
  bookdown::word_document2: default
  rticles::arxiv_article: default
  bookdown::pdf_book:
    base_format: rticles::arxiv_article
---

```{r include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


# Introduction {-}

Over the past decade a feeling that there is a crisis in experimental science has increasingly permeated the thinking of methodologists, captains of industry, working scientists, and even the lay public [@ioannidis2005most;@baker2016; @begley2012; @prinz2011believe;@harris2017rigor]. This manifests in poor statistical power to find true effects [@button2013power], in poor reproducibility (defined as getting identical results when reanalysing the original data by the original analytic workflow), and in poor replicability (defined as getting similar results after repeating the entire experiment) of the results [@goodman2016]. While reproducibility depends on the availability of data and on the quality of description of the data analytic workflow, replicability depends on the quality of experiments, on the quality of data analysis, including data pre-processing, and on the power of the experiment to detect true effects.
The proposed reasons behind the crisis include sloppy experimentation, selective publishing, perverse incentives, difficult-to-run experimental systems, insufficient sample sizes, over-reliance on null hypothesis testing, and much-too-flexible analytic designs combined with hypothesis-free study of massively parallel measurements [@grimes2018; @maivali2015interpreting; @munafo2017manifesto; @szucs2017null;@botvinik2020variability; @leng2020matter]. Although there have been attempts at assessing experimental quality through replication of experiments, prohibitive cost and theoretical shortcomings in analysing concordance in experimental results have encumbered this approach [@hardwicke2020calibrating]. 
Here we assess *in silico* the reproducibility and replicability of high throughput differential expression studies by next-generation sequencing (HT-seq). We concentrate on the HT-seq field for two reasons. HT-seq has become the gold standard for whole transcriptome gene expression quantification, both in research and in clinical applications. And secondly, due to the massively parallel testing in individual studies of tens of thousands of features per experiment, we have access to study-wide unbiased lists of p values. From the shapes of histograms of p values we can find the experiments where p values were calculated apparently correctly, and from these studies we can determine the study-wise relative frequencies of true nulls (the $\pi_0$-s). Also, we believe that the very nature of the HT-seq field, where a single biological experiment entails comparing the expression levels of about 20,000 features (e.g. RNA-s) on average, predicates that the quality of data analysis, and specifically statistical inference based on p values (directly, or indirectly through FDR) must play a decisive part in scientific inference. Simply, one cannot analyse an HT-seq experiment intuitively, without resorting to formal statistical inference. Therefore, quality problems of statistical analysis would very likely directly and substantially impact the quality of science. Thus we use the quality of statistical analysis as a proxy for the quality of science, with the understanding that this proxy may work better for modern data-intensive fields, where scientist's intuition has a comparatively smaller role to play.


# Results {-}

## Assessing reproducibility by NCBI GEO database supplementary files {-}

We queried the NCBI GEO database for "expression profiling by high throughput sequencing", retrieving  32,017 datasets (GEO series) from 2006, when first HT-seq dataset was submitted to GEO, to Dec-31, 2019. The number of yearly new HT-seq submissions increased from 1 in 2006 to 8770 by 2019, making up 27.4% of all GEO submissions in 2019. Most of the GEO HT-seq submissions are from human and mouse, and they increase in a much faster rate than submissions from other taxa (data not shown). 
We filtered the GEO series containing supplementary processed data files. Processed data are a required part of GEO submissions, defined as the data on which the conclusions in the related manuscript are based. The format of processed data files submitted to GEO is not standardized, but in case of expression profiling such files include, but are not limited to, quantitative data for features of interest, e.g. mRNA, in tabular format. Sequence read alignment files and coordinates (SAM, BAM, and BED) are not considered as processed data by GEO. 
As reproducibility by definition entails arriving at the original conclusions by independently repeating original analysis [@peng2011reproducible], we surmise that the fraction of GEO submission with processed data files gives an upper limit of reproducibility (upper limit, because processed data files *per se* do not guarantee reproducibility).  According to our analysis 17,920 GEO series, containing 43,340 supplementary processed data files, conform with GEO submission guidelines. After further excluding the submissions with potentially non-tabular supplementary files, based on file extensions (see Methods for details), the number of GEO series was reduced to 15,520, containing 31,862 supplementary files, including tar.gz archives, which we downloaded from the GEO server.  From those we programmatically imported 32,764 files as tables, resulting in 32,414 (99%) successfully imported files.  For the purpose of reproducibility analysis, we considered all 17,920 GEO series with supplementary processed files out of 32,017 published GEO series in our time window as potentially reproducible.  Therefore, the observed overall 56% retention rate of GEO submissions with processed data files should be seen as an upper bound for reproducibility. There is a substantial increase of the retention rate over time, from 16% (3/19) in 2008 to 62% (5,465/8,770) in 2019, indicating increasing reproducibility of the HT-seq field (Figure \ref{fig:fig1}).

(ref:fig1) The increasing proportion of GEO submissions conforming with submission guidelines in regard of inclusion of processed data is estimated from a bernoulli  logistic model. Line denotes linear model best fit using formula *conforms $\sim$ year*, bernoulli likelihood. Shaded area denotes 95% credible region. N = 32,017.

```{r fig1, fig.cap="(ref:fig1)", out.width="8cm", fig.align='center'}
knitr::include_graphics(here::here("figures/figure_1.pdf"))
```


According to GEO submission requirements, the processed data files may contain raw counts of sequencing reads, and/or normalized abundance measurements. Therefore, a valid processed data submission may or may not contain lists of p values. We identified p values from 2,109 GEO series, from which we extracted 6,267 unique p value sets. While the mean number of p value lists, each list corresponding to a separate experiment,  per 2,109 GEO submissions was 2.97 (max 66), 49% of submissions contained a single p value list and 78% contained 1-3 p value lists. For further analysis we randomly selected one p value list per GEO series.


## P value histograms {-}

(ref:fig2) Classes of p value histograms. A. Summary of p value histograms identified from GEO supplementary files. One p value set was randomly sampled from each GEO series where p values were identified. N = 2,109. 95% CI denotes credible intervals calculated from a binomial model. B. Examples of p value histogram classes. Red line denotes QC threshold used for dividing p value histograms into discrete classes.

```{r fig2, fig.cap="(ref:fig2)", fig.align="center", out.width="12cm"}
knitr::include_graphics(here::here("figures/figure_2.pdf"))
```

   
We algorithmically classified the p value histograms into five classes [@breheny2018p] (See materials and methods for details and Figure \ref{fig:fig2}  for representative examples). The "Uniform" class contains flat p value histograms indicating no true effects (at the sample sizes used to calculate these p values). The "Anti-Conservative" class contains otherwise flat histograms that contain a spike near zero. The "Conservative" class contains histograms that have a distinct spike close to one. The "Bimodal" histograms have two peaks, one at either end. The class "Other" contains a panoply of malformed histogram shapes (humps in the middle, gradual increases towards one, spiky histograms, etc.). The "Uniform" and "Anti-Conservative" histograms are the theoretically expected shapes of p value histograms.

(ref:fig3) Association of the p value histogram class with differential expression analysis tool. A. The increase in the proportion of anti-conservative histograms is accompanied by decreases mostly in the class "other", irrespective of the DE analysis tool. Lines denote best fit of linear model *class $\sim$ year + (year | de_tool)*, categorical likelihood. Shaded areas denote 95% credible regions. N = 2,109. B. Association of p value histogram type with DE analysis tool; data is restricted to 2018-2019 GEO submissions. Points denote linear model fit *class $\sim$ de_tool*, categorical likelihood. Error bars denote 95% credible intervals. N = 980.

```{r fig3, fig.align='center', out.width="14cm", fig.cap="(ref:fig3)"}
knitr::include_graphics(here::here("figures/figure_3.pdf"))
```


We found that overall, 23% of the histograms fall into anti-conservative class, 12% were conservative, 30% bimodal and 36% fell into class "other". Only 3 of the 2,109 histograms were classified as "uniform". Logistic regression reveals a clear trend for increasing proportion of anti-conservative histograms, starting from <10% in 2010 and topping 25% in 2018 (Figure 3--figure supplement 1). Hierarchical modelling indicates that all differential expression (DE) analysis tools and sequencing platforms exhibit similar temporal increases of anti-conservative p value histograms (Figure 3--figure supplement 2-3). Multinomial hierarchical logistic regression further demonstrated that the increase in the fraction of anti-conservative histograms is accomplished by decreases mostly in the class "other", irrespective of the DE analysis tool (Figure \ref{fig:fig3}A).

This positive temporal trend in anti-conservative p value histograms suggests improving quality of the HT-seq field. Rather surprisingly, Figure \ref{fig:fig3}A also indicates that different DE analysis tools are associated with very different proportions of p value histogram classes, suggesting that quality of p value calculation, and therefore, quality of scientific inferences based on these p values depends on DE analysis tool. We further tested this conjecture in a simplified model, restricting our analysis to 2018-2019, the final years in our dataset (Figure \ref{fig:fig3}B). As no single DE analysis tool dominates the field -- cuffdiff 23%, deseq 33%, edgeR 13%, limma 2%, unknown 29% (see Figure 3--figure supplement 4 for temporal trends) --, a state of affairs where proportions of different p value histogram classes do not substantially differ between analysis tools would indicate lack of DE analysis tool-specific bias to the results. However, we find by multinomial regression that all p value histogram classes, except "uniform", which is largely unpopulated, depend strongly on the DE analysis tool used to calculate the p values (Figure \ref{fig:fig3}B). This is confirmed by modelling the frequency of the anti-conservative p value histograms in binomial logistic regression (Figure 3--figure supplement 5A). Using the whole dataset of 6,267 p value histograms, or controlling for GEO publication year, taxon (human, mouse, and pooled other), of the RNA source or sequencing platform does not change this conclusion (Figure 3--figure supplement 5B-E). These results indicate that DE analysis tools bias the analysis of HT-seq experiments. 

## Proportion of true nulls {-}

To further enquire into DE analysis tool-driven bias we estimated from user-submitted p values the fraction of true null effects (the $\pi_0$) for each HT-seq experiment. As non-anti-conservative sets of p values (excepting the "uniform") indicate problems during the respective experiments and/or data analyses, we only calculated the $\pi_0$ for datasets with anti-conservative and uniform p value distributions (n = 488). Nevertheless, the $\pi_0$-s show an extremely wide distribution, ranging from 0.999 to 0.06. Remarkably, 36% of the $\pi_0$ values are smaller than 0.5, meaning that in those experiments over half of the features (e.g. mRNA-s) are estimated to change their expression levels upon experimental treatment (Figure \ref{fig:fig4}A). Conversely, only 21% of $\pi_0$-s exceed 0.8, and 8.5% exceed 0.9. Intriguingly, the peak of the $\pi_0$ distribution is not near 1, as might be expected from experimental design considerations, but there is a wide peak between 0.5 and 0.8 (median and mean $\pi_0$-s are both at 0.59). The median $\pi_0$-s range over 20 percentage points, from 0.5 to 0.7, depending on the DE analysis tool (Figure \ref{fig:fig4}B). Using the whole dataset confirms this analysis, also producing narrower credible intervals, due to larger sample (N = 1,567) (Figure 4--figure supplement 1A).

(ref:fig4) Association of the proportion of true null effects ($\pi_0$) with DE analysis tool. A. Histogram of $\pi_0$ values estimated from anti-conservative and uniform p value sets. N = 488. B. Robust linear model (*pi0 $\sim$ de_tool*, student's t likelihood) indicates association of $\pi_0$ with DE analysis tool. Points denote best estimate for the mean $\pi_0$ and error bars denote 95% credible intervals. N = 488. C. Hierarchical model (*pi0 $\sim$ year + (year | de_tool)*, student's t likelihood) reveals that all DE analysis tools are associated with temporally increasing proportion of true null effects. Lines denote best estimate for the mean $\pi_0$ and error bars denote 95% credible regions. N = 488.

```{r fig4, fig.align='center', out.width="14cm", fig.cap="(ref:fig4)"}
knitr::include_graphics(here::here("figures/figure_4.pdf"))
```

In addition, mean $\pi_0$ tend to rise in time, similarly to fraction of anti-conservative p value histograms, and this increase is also common to all DE analysis tools (Figure \ref{fig:fig4}C). Controlling for time, taxon or sequencing platform, did not substantially change the association of DE analysis tools with the $\pi_0$-s, except for the multilevel models, which resulted in substantially larger estimation uncertainty (Figure 4--figure supplement 1B-E). Recalculating the $\pi_0$-s with a different algorithm [@storey2002direct] and reanalysing the data did not change these conclusions (data not shown).
As there is a strong association between both proportion of anti-conservative p value histograms and $\pi_0$ with DE analysis tool, we further checked for, and failed to see, similar associations with variables from raw sequence metadata, such as the sequencing platform, library preparation strategies, library sequencing strategies, library selection strategies, and library layout (single or paired) (Figure 3--figure supplement 6-9, Figure 4--figure supplement 2-5). These negative results support the conjecture of specificity of the associations with DE analysis tools.

# Discussion {-}

In this work we have calculated five indicators of health of the HT-seq field: (i) the proportion of GEO submissions with published analysis endpoints in a tabular format, (ii) the relative fractions of classes of p value distribution shapes, (iii) the association of p value distribution shapes with DE analysis tool, (iv) the estimated proportion of true null effects (the $\pi_0$ ) for each experiment, and (v) the association of $\pi_0$ values with DE analysis tool. We believe that (i) indicates reproducibility, (ii) the quality of p value calculations (use of correct distributional models, etc.), (iv) the experimental design and data pre-processing choices, and that (iii) and (v) indicate bias. 

Our analysis puts an upper bound of about 60% to differential HT-seq reproducibility in recent years, based on the presence of processed data files in the GEO submissions. To elucidate the meaning of this estimate, we must look into the relationship between the GEO submission data structures and the HT-seq workflows. The GEO repository requires three components for data submission: a metadata spreadsheet providing experimental design details, tabular processed data files, defined as the data on which the conclusions in the related manuscript are based (containing normalized abundance measurements or raw counts of sequencing reads), and raw data files (reads and quality scores as generated by the sequencing instrument). Even in the presence of raw data files, processed data files are practically necessary for assessing and reproducing the evidence behind the conclusions of a study, because of the large number of choices available to a discerning analyst. To put this in another way: scientific inference in the functional genomics field is based on null hypothesis testing and p values, and there isn't a single correct method to calculate them (nor is there one for FDR). Therefore, there is no guarantee that two independent analyses of the same data will obtain the same p values, and thus the same conclusions [@maivali2015interpreting]. This means that access to original analytic choices (analysis code), or at least to p values, is needed for reproducibility. 

The major points of divergence in the analytic pathway include aligning the sequences to genomic DNA, counts normalization, and differential expression testing [@nookaew2012comprehensive;@sun2012systematic]. The analysis tools that are used for differential expression testing allow for a plethora of choices, including different distributional models, data transformations and basic analytic strategies, which can lead to different results through different trade-offs [@everaert2017benchmarking; @nookaew2012comprehensive]. A similar state of affairs exists in the field of fMRI experiments, where reproducibility is questionable, and where 70 teams, testing the same hypotheses on the same data, used 70 different workflows, getting variable results [@carp2012plurality; @carp2012secret;@botvinik2020variability]. 
Taken together, this leads us to conclude that access to correctly annotated read counts is a minimum requirement for assessing reproducibility of read pre-processing and alignment. More desirably, also raw p values are necessary to judge the evidence behind conclusions of a paper. For full reproducibility, starting from raw sequencing data, complete analysis instructions and modelling choices should be provided, which is currently not a part of the GEO submission protocol. Thus our reproducibility estimate does not refer to full reproducibility, but to mere potential of independently recreating conclusions of a paper. Although full reproducibility of the HT-seq field still seems elusive, the robust temporal trend of improvement, documented by us, gives reason for optimism.

While there is also a positive trend for the increasing fraction of anti-conservative p value sets, a strong majority of them yet fall into shapes that are considered problematic for successful analysis, including FDR/q value calculations. In fact, the most common class of p value histograms, "other", encompasses a diverse mixture of unruly shapes least likely to lead to good downstream analysis and interpretation of these p values. 

There are very few uniform p value distributions, suggesting relatively few true effects. This surprising result was confirmed by visual re-examination of p value histograms. As a technical comment, it should be noted, that the assigned class of the p value histogram depends on arbitrarily set bin size. Our use of 40 bins leads to histograms, where an experiment with even around 100 true effects could reasonably lead to uniform histogram shape, because of swamping of the lowermost bin with p values emanating from true null effects (see Figure 2--figure supplement 1).

Importantly, the proportions of different classes of p value distributions differ greatly between DE analysis tools, indicating analysis tool-specific bias. We see similar tool-specific bias (and similar temporal increase) in the value of estimated fractions of true null effects ($\pi_0$). As $\pi_0$ -s were calculated from anti-conservative p value sets only, this bias could mean (i) that the anti-conservativeness of a p value set is not a sufficient predictor of its quality in terms of further analysis, and/or (ii) that the algorithms implemented in differential expression analysis tools differ to a degree that introduces such bias downstream of the p value calculation. 

The DE analysis tool-specific medians of $\pi_0$ values range from 0.43 (tool "unknown") to 0.68 (tool "cuffdiff"), and the total median $\pi_0$ is 0.52, showing that by this criterion in an average experiment about half of the cellular RNA-s are expected to change their expression levels. 

In a biased situation the measured effects are a mixture of effects of the intended experimental treatment and of the undesirable effects of a more-or-less accidental choice of DE analysis tool, as no analytic workflow has been shown to systematically outperform the others [@everaert2017benchmarking; @nookaew2012comprehensive]. Determining the relative weight of DE analysis tool in this mixture requires careful case-by-case study, as this depends both on the performed experiment (its variations, effect sizes, and actual laboratory implementation), and on the particular analytic choices reflected in the shapes of p value distributions and in $\pi_0$ values. However, from the evidence at hand, notably from the strong associations with DE analysis tool, in combination with the preponderance of extreme p value distributions and low $\pi_0$-s, we can conclude that such bias must be substantial, as it is widespread. 
Another limitation of our study, also due to its large-scale nature, is our inability to pinpoint the sources of DE analysis tool-specific bias. However, a recent close study of 35 datasets shows data normalization to be a potentially important source of bias, which cannot be corrected by many of the currently widely used data normalization methods [@mandelboum2019recurrent;@quinn2019field]. Indeed, the widely used DE analysis tools use different RNA-seq data pre-processing strategies, all vulnerable to the situation where a large fraction of features change expression [@mcgee2019compositional]. In the light of our findings, the sources of bias clearly merit further case-by-case study of individual GEO submissions. 

Could the relatively high frequency of low  $\pi_0$-s reflect reality in the sense that in many experiments most RNA-s actually change expression levels? Although there is a small number of well-supported examples of this [@lin2012transcriptional;@nie2012c;@hu2014nucleosome], it has been argued that the vast majority of genome wide differential gene expression studies ever conducted, including by HT-seq, have used experimental designs that would make it impossible to uncover such global effects, at least in a qualitatively accurate way [@loven2012revisiting;@chen2016overlooked]. The issue lies in the use of internal standards in normalizing the counts for different RNA-s (commonly normalizing for total read counts in each sample), which leads to completely wrong interpretations, if most genes change expression in one direction. To overcome this problem, one could use spike-in RNA standards like the External RNA Controls Consortium (ERCC) set (Ambion) or the Spike-in RNA Variants (SIRV) set (Lexogen) [@lun2017assessing] and compositional normalization [@mcgee2019compositional]. However, even spike-in normalization requires great care to properly work in such extreme cases [@risso2014role;@quinn2019field], and outside single-cell RNA-seq it is used infrequently [@mcgee2019compositional]. In the absence of spike-in normalization, it seems likely that many or most of the low $\pi_0$ experiments represent technical failures, most likely during data normalization [@mcgee2019compositional]. 

While the aim of this paper was to provide a birds-eye view on the HT-seq field, the dataset created to support this work provides added value by allowing access at individual GEO submission/experiment level. The dataset that accompanies this study allows to get a first estimate for the reproducibility and quality of conclusions of more than 30,000 HT-seq studies deposited in NCBI GEO. 
The dataset contains for every GEO submission the date of submission, association with publications, organisms studied, association with tabular files; and for every submitted tabular file it contains the name of file, the number of p value columns, the number of features, the p value histogram type and depiction of the histogram, $\pi_0$.    
Finally, we note that our methodology can be adapted to study any type of experiment that results in at least several hundreds of parallel measurements/ p values, such as quantitative protein mass spectroscopy and metabolomics.   


Strengths of the study:

-	Our study is based on a large unbiased dataset, which allows for reliable quantification. 
-	All steps of the analysis are transparent and reproducible, as we provide full workflow and code for data mining, p value histogram classification, $\pi_0$ calculations, modelling, and figures. 
-	To the best of our knowledge this is the first large-scale study to offer quantitative insight into general quality of experimentation/data analysis/reproducibility of a fairly large field of biomedical science. 
-	The accompanying full dataset includes, for individual GEO submissions, useful quality control measures, like presence of tabular files describing analysis end-points, the shapes of the p value distributions and the estimated proportions of true null effects, providing evidence on the quality of inference of almost any HT-seq work in the literature.


Limitations of the study:

-	It is a general study, whose measure of bias (statistical association inferred from many experiments) cannot be directly extended to single experiments. To do so requires (at least) incorporating additional information about the effect sizes encountered in the study of interest. 
-	From our dataset we cannot determine, which features of which differential expression analysis tools are responsible for the inferred bias. Neither can we say, which (if any) of the tools are better than the others. However, these questions have been addressed in the literature, and we hope that by stressing the seriousness of the problem, our study will inspire further work on the subject.
-	Our estimate for reproducibility only provides an upper limit (presence of tabular files, which have the mere potential of containing enough pertinent information). Obtaining the true level of reproducibility requires actual reproduction of individual analyses, which we feel, cannot be automated. Although presence of tabular supplementary files is not a good measure of reproducibility of an individual study, their absence give a strong indication for irreproducibility of a study.

# Methods {-}

NCBI GEO database queries were performed using Bio.Entrez Python package and by sending requests to NCBI Entrez public API. The query was 'expression profiling by high throughput sequencing[DataSet Type] AND ("2000-01-01"[PDAT] : "2019-12-31"[PDAT])'. FTP links from GEO datasets document summaries were used to download supplementary file names. Supplementary file names were filtered for downloading, based on file extensions, to keep file names with "tab", "xlsx", "diff", "tsv", "xls", "csv", "txt", "rtf", and "tar" file extensions. We dropped the file names where we did not expect to find p values using regular expression "filelist.txt|raw.tar\$|readme|csfasta|(big)?wig|bed(graph)?|(broad_)?lincs". 

Downloaded files were imported using Python pandas package, and searched for unadjusted p value sets. Unadjusted p value sets and summarized expression level of associated genomic features were identified using column names. P value columns from imported tables were identified by regular expression "p[^a-zA-Z]{0,4}val", from these, adjusted p value sets were identified using regular expression "adj|fdr|corr|thresh" and omitted from further analysis. Columns with expression levels of genomic features were identified by using following regular expressions: "basemean", "value", "fpkm", "logcpm", "rpkm", "aveexpr". 

Raw p value sets were classified based on their histogram shape. Histogram shape was determined based on the presence and location of peaks. P value histogram peaks (bins) were detected using a quality control threshold described in [@breheny2018p], a Bonferroni-corrected alpha-level quantile of the cumulative function of the binomial distribution with size m and probability p. Histograms, where none of the bins were over QC-threshold, were classified as "uniform". Histograms, where bins over QC-threshold started either from left or right boundary and did not exceeded 1/3 of the 0 to 1 range, were classified as "anti-conservative" or "conservative", respectively. Histograms with peaks or bumps in the middle or with non-continuous left- or right-side peaks were classified as "other". Histograms with peaks on both left- and right-side were classified as "bimodal". 

Raw p value sets with anti-conservative shape were used to calculate the $\pi_0$ statistic. The $\pi_0$  statistic was calculated using local FDR method implemented in limma::PropTrueNullByLocalFDR [@ritchie2015limma] and, independently, Storey's global FDR smoother method [@storey2002direct] as implemented in gdsctools [@cokelaer2017] Python package. Differential expression analysis tools were inferred from column names pattern for cuffdiff (column name = "fpkm" and "p_value") [@trapnell2013], DESeq/DESeq2 (column name = "basemean") [@love2014], EdgeR (column name = "logcpm") [@mccarthy2012], and limma (column name = "aveexpr") [@ritchie2015], all other unidentified sets were binned as "unknown". 

Publication data were downloaded from NCBI PubMed database using PubMedId-s from GEO document summaries. Citation data were downloaded from Elsevier Scopus database using doi-s. Sequence read library statistics were downloaded from NCBI SRA database using GEO accessions. 

The code to produce raw dataset is available as a snakemake workflow [@koster2012snakemake] on rstats-tartu/geo-htseq Github repo (https://github.com/rstats-tartu/geo-htseq). Raw dataset produced by the workflow is deposited in Zenodo https://zenodo.org with doi: 10.5281/zenodo.4046422 (http://doi.org/10.5281/zenodo.4046422). 

RNA-seq experiment simulation was done with polyester R package [@frazee2015] and differential expression was assessed using DESeq2 R package [@love2014] using default settings. Code and workflow used to run and analyse RNA-seq simulations is available on Github: https://github.com/rstats-tartu/simulate-rnaseq. Processed data, raw data and workflow with input fasta file is deposited in Zenodo with doi: 10.5281/zenodo.4463803 (http://doi.org/10.5281/zenodo.4463803).

The code to produce article's figures and fit models is deposited on rstats-tartu/geo-htseq-paper Github repo (https://github.com/rstats-tartu/geo-htseq-paper). Article's input data, code, software required to produce all models and figures in Linux, along with fitted model objects is deposited in Zenodo with doi: 10.5281/zenodo.4469911 (https://zenodo.org/deposit/4469911). 
Statistical analysis was done in R vers. 4.0.3. Bayesian modelling was done using R libraries rstan (vers. 2.21.2; http://mc-stan.org/) and brms (vers. 2.13.3) [@burkner2018]. ggplot2 (vers. 3.3.1) [@wickham2016ggplot2] R library was used for graphics. Data wrangling was done using tools from tidyverse package [@wickham2019].

# Acknowledgments {-}

We are grateful for Toomas Mets (University of Tartu) for critically reading the manuscript, and Niilo Kaldalu (University of Tartu) and Margus Pihlak (Tallinn University of Technology) for useful discussions. The work was supported by the European Union from the European Regional Development Fund through the Centre of Excellence in Molecular Cell Engineering (2014-2020.4.01.15-0013) and by the grants from the Estonian Research Council (PRG335, PUT1580).

# References {-}
